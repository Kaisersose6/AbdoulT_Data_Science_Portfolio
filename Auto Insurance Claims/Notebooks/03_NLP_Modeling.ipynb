{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce07010",
   "metadata": {},
   "source": [
    "# NLP Modeling (BERT on Claim Descriptions)\n",
    "\n",
    "## ✅ **GOAL**\n",
    "\n",
    "Build a binary classification model using BERT for natural language inputs (e.g., claim descriptions). The model predicts the probability of a positive class (e.g., fraud = 1).\n",
    "\n",
    "### 📦 Import Required Libraries\n",
    "\n",
    "TFBertModel: Pretrained BERT model from HuggingFace in TensorFlow.\n",
    "\n",
    "BertTokenizer: For tokenizing raw text into BERT inputs (input_ids, attention_mask).\n",
    "\n",
    "Keras layers: To build the neural network.\n",
    "\n",
    "Lambda: To wrap BERT inside a custom functional layer.\n",
    "\n",
    "### 🤖 Load Pretrained BERT Model\n",
    "\n",
    "Downloads and loads BERT-base (uncased) pretrained on general English text (Wikipedia + BookCorpus).\n",
    "\n",
    "It outputs a hidden state for each token in the input, as well as the pooled output from the [CLS] token.\n",
    "\n",
    "### 🧩 Define Input Placeholders\n",
    "\n",
    "input_ids: Encoded token IDs from text input (max 128 tokens).\n",
    "\n",
    "attention_mask: Indicates which tokens are actual input (1) and which are padding (0).\n",
    "\n",
    "### 🧠 Custom Function to Get [CLS] Token\n",
    "\n",
    "The [CLS] token at position 0 is typically used in BERT for sentence-level classification.\n",
    "\n",
    "last_hidden_state[:, 0, :] extracts that 768-dimensional vector from BERT's output.\n",
    "\n",
    "#### 🔁 Wrap BERT in Lambda Layer\n",
    "\n",
    "This is necessary because Keras can’t automatically infer output shape from HuggingFace models.\n",
    "\n",
    "Output is the [CLS] vector (shape: (batch_size, 768)), which contains the semantic summary of the input sentence.\n",
    "\n",
    "### 🧱 Add Classification Layers\n",
    "\n",
    "Dropout: Prevents overfitting by randomly dropping 30% of neurons.\n",
    "\n",
    "Dense(128, relu): Learns non-linear representations.\n",
    "\n",
    "Dense(1, sigmoid): Outputs a probability between 0 and 1—used for binary classification.\n",
    "\n",
    "### 🧪 Compile the Model\n",
    "\n",
    "Inputs: BERT-formatted data (input_ids, attention_mask)\n",
    "\n",
    "Output: Probability of the positive class (e.g., fraud = 1)\n",
    "\n",
    "Loss: Binary cross-entropy (standard for 2-class problems)\n",
    "\n",
    "Optimizer: Adam (adaptive learning)\n",
    "\n",
    "### 📋 Print Model Architecture\n",
    "\n",
    "This prints a table with:\n",
    "\n",
    "Input and output shapes of each layer\n",
    "\n",
    "Number of trainable parameters (BERT has ~110M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d42b9225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Text Data\n",
    "import pandas as pd \n",
    "\n",
    "text_data = pd.read_csv('../data/text_data.csv')\n",
    "\n",
    "# Import y data\n",
    "y = pd.read_csv('../data/y.csv')\n",
    "\n",
    "#Import X structured data\n",
    "X_structured = pd.read_csv('../data/X_structured.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7f0016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum sequence length (typically 128 or 512 for BERT)\n",
    "max_length = 128  # You can change this based on your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8daa8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_mask      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bert_cls_output     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bert_cls_output[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dense_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_mask      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bert_cls_output     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bert_cls_output[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m98,432\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dense_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">98,561</span> (385.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m98,561\u001b[0m (385.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">98,561</span> (385.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m98,561\u001b[0m (385.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['../data/model.pkl']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFBertModel, BertTokenizer\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load pretrained BERT model\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define inputs\n",
    "input_ids = Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = Input(shape=(max_length,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# Wrap BERT in Lambda layer with output_shape defined\n",
    "def extract_bert_cls(inputs):\n",
    "    ids, mask = inputs\n",
    "    outputs = bert_model([ids, mask])\n",
    "    return outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "# Define output shape: (batch_size, hidden_size)\n",
    "cls_output = Lambda(extract_bert_cls, output_shape=(768,), name=\"bert_cls_output\")([input_ids, attention_mask])\n",
    "\n",
    "# Add classification layers\n",
    "x = Dropout(0.3)(cls_output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Show model summary\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b3a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_structured)  # Replace with your actual input\n",
    "dump(predictions, \"bert_predictions.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a67c0663",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_data['claim_description'].tolist()  # Replace with your text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df58e378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 907ms/step - accuracy: 0.7609 - loss: 0.5439 - val_accuracy: 0.7900 - val_loss: 0.4072\n",
      "Epoch 2/3\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 898ms/step - accuracy: 0.7778 - loss: 0.4755 - val_accuracy: 0.7700 - val_loss: 0.3973\n",
      "Epoch 3/3\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 876ms/step - accuracy: 0.7810 - loss: 0.4553 - val_accuracy: 0.7800 - val_loss: 0.3940\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=128, return_tensors='tf')\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    x={'input_ids': encodings['input_ids'], 'attention_mask': encodings['attention_mask']},\n",
    "    y=y['fraud_reported'],  # Replace with your target column\n",
    "    batch_size=32,\n",
    "    epochs=3,\n",
    "    validation_split=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd03471e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 839ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict({\n",
    "    \"input_ids\": encodings[\"input_ids\"],\n",
    "    \"attention_mask\": encodings[\"attention_mask\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fccd780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(\"../data/bert_predictions.npy\", predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
